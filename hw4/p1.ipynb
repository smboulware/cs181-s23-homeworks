{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CS 181 HW4 Problem 1**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize data and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a specific example of when we have $K = 3$ component Gamma distributions. Let's initialize the initial parameter values for $\\theta$ and $\\beta_k$ as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\theta_k &=  1/K, \\\\\n",
    "  \\beta_k & = k/K.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that we usually initialize $\\theta$ and $\\beta_k$ randomly. However, by fixing the initial $\\theta$ and $\\beta_k$, EM becomes deterministic which makes debugging (and grading) easier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as ds\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "x = torch.load('data.pt')\n",
    "\n",
    "K = 3\n",
    "theta = torch.ones(K) / K\n",
    "alpha = 5.0\n",
    "betas = (torch.arange(K) + 1) / K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import gamma\n",
    "x = x.numpy()\n",
    "theta = theta.numpy()\n",
    "betas = betas.numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** implement the E-step (Use expression derived in part 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(theta, betas):\n",
    "    q = 'not implemented'\n",
    "    return q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** implement the M-step (Use expressions derived in parts 4b and 4c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(q):\n",
    "    theta_hat = 'not implemented'\n",
    "    beta_hats = 'not implemented'\n",
    "    return theta_hat, beta_hats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Likelihood Implemented Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_px(x, theta, betas):\n",
    "    log_pxs = gamma.logpdf(x, alpha, scale=1/betas) + np.log(theta)\n",
    "    \n",
    "    return logsumexp(log_pxs, axis=1)\n",
    "\n",
    "def log_likelihood(theta, betas):\n",
    "    return log_px(x, theta, betas).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** implement EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_em(theta, betas, iterations=1000):\n",
    "    theta = 'not implemented'\n",
    "    betas = 'not implemented'\n",
    "    return theta, betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(theta, betas):\n",
    "    x_test = torch.linspace(0.01, x.max(), 1000)\n",
    "    prob = log_px(x_test.unsqueeze(-1), theta, betas).exp()\n",
    "    # prob = np.exp(log_px(x_test.unsqueeze(-1), theta, betas))  # use this line for numpy\n",
    "    ll = log_likelihood(theta, betas)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    \n",
    "    fig.subplots_adjust(top=0.7)\n",
    "    fig.suptitle(f'theta = {theta}\\nbeta = {betas}\\nlog likelihood = {ll:.3e}')\n",
    "\n",
    "    ax1.set_title('Dataset')\n",
    "    ax1.hist(x.T, bins=100, color='tomato')\n",
    "    # ax1.hist(x, bins=100, color='tomato')  # use this line for numpy\n",
    "    ax2.set_title('Gamma mixture')\n",
    "    ax2.plot(x_test, prob, color='tomato')\n",
    "    plt.savefig('p1.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, betas = run_em(theta, betas)\n",
    "make_plot(theta, betas)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c365ea26a318e0b540d1978e97e3d03cfe51dff8cd04dae5f3d7b47d79d2453"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
